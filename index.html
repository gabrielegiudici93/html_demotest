<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Dexterous Manipulation, Machine Learning For Robot Control, Deep Learning in Grasping and Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DexSkills</title>
  <link rel="icon" type="image/x-icon" href="static/images/First_image_lightC.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Your CSS styles continue below... */
    /* Set width of all images to 100% for responsiveness */
    img {
      width: 100%;
      height: auto;
    }

    /* Ensure all text content has the same width */
    .container {
      max-width: 800px; /* Adjust as needed */
    }

    /* Ensure all columns have the same width */
    .column {
      width: 100%;
    }

    /* Your existing CSS styles continue below... */
    body {
      font-family: 'Noto Sans', sans-serif;
    }

    .footer .icon-link {
      font-size: 25px;
      color: #000;
    }

    .link-block a {
      margin-top: 5px;
      margin-bottom: 5px;
    }

    .dnerf {
      font-variant: small-caps;
    }

    .teaser .hero-body {
      padding-top: 0;
      padding-bottom: 3rem;
    }

    .publication-title {
      font-family: 'Google Sans', sans-serif;
    }

    .publication-authors {
      font-family: 'Google Sans', sans-serif;
      color: #ff3860;
      width: fit-content;
      font-weight: bolder;
    }

    .publication-venue {
      color: #555;
      width: fit-content;
      font-weight: bold;
    }

    .publication-authors a {
      color: hsl(204, 86%, 53%) !important;
    }

    .publication-authors a:hover {
      text-decoration: underline;
    }

    .publication-video {
      position: relative;
      width: 100%;
      height: 0;
      padding-bottom: 56.25%;
      overflow: hidden;
      border-radius: 10px !important;
    }

    .publication-video iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    .publication-body img {
      max-width: 100%;
      height: auto;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DexSkills: Skill Segmentation Using Haptic Data for Learning Autonomous Long-Horizon Robotic Manipulation Task</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/xiaofeng-mao-45449a202/" target="_blank">Xiaofeng Mao</a><sup>1,*</sup>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/gabrielegiudici93/" target="_blank">Gabriele Giudici</a><sup>2,*</sup>,</span>
              
                  <span class="author-block">
                  <a href="https://www.claudiocoppola.com/" target="_blank">Claudio Coppola</a><sup>3</sup>,</span>
                  <span class="author-block">
                  <a href="https://www.sems.qmul.ac.uk/staff/k.althoefer" target="_blank">Kaspar Althoefer</a><sup>2</sup>,</span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/ildar-farkhatdinov" target="_blank">Ildar Farkhatdinov</a><sup>2</sup>,</span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/zhibin-alex-li" target="_blank">Zhibin Li</a><sup>4</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.sems.qmul.ac.uk/staff/l.jamone" target="_blank">Lorenzo Jamone<sup>2</sup></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Edinburgh <sup>1</sup>,Queen Mary University of London<sup>2</sup>, Amazon ATS<sup>3</sup>, University College London<sup>4</sup><br>Submitted IROS 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ARQ-CRISP/DexSkills" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/IROS_V3.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Effective execution of long-horizon tasks with dexterous robotic hands remains a significant challenge in real-world problems. While learning from human demonstrations have shown encouraging results, they require extensive data collection for training. Hence, decomposing long-horizon tasks into reusable primitive skills is a more efficient approach. To achieve so, we developed DexSkills, a novel supervised learning framework that addresses long-horizon dexterous manipulation tasks using primitive skills. DexSkills is trained to recognize and replicate a select set of skills using human demonstration data, which can then segment a demonstrated long-horizon dexterous manipulation task into a sequence of primitive skills to achieve one-shot execution by the robot directly. Significantly, DexSkills operates solely on proprioceptive and tactile data, i.e., haptic data. Our real-world robotic experiments show that DexSkills can accurately segment skills, thereby enabling autonomous robot execution of a diverse range of tasks.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Figures displayed one above the other -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item has-text-centered">
        <!-- First image -->
        <img src="static/images/First_image_lightC.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle">
          Overview of the proposed long-horizon task segmentation approach. Individual skills are segmented and classified at each temporal window of the demonstration.
          The demonstrations are collected via the teleoperation system developed in [15].
        </h2>
      </div>
      
      <div class="item has-text-centered">
        <!-- Second image -->
        <img src="static/images/network_deep_finaL_v3_B.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle">
          The leader agent generates motor control commands for the end effector pose and finger joints of the hand. The follower robot executes corresponding actions based on these commands. During teleoperation, the follower robot provides haptic feedback. 
          When operating the robot autonomously, we control the robot using a distinct MLP trained on the proprioceptive and tactile data (i.e. haptic data) of each separate skill.
        </h2>
      </div>
      
      <div class="item has-text-centered">
        <!-- Third image -->
        <img src="static/images/network_deep_finaL_v3_A.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle">
          The architecture of our Neural Network for supervised representation learning incorporates an auto-regressive autoencoder and a label decoder.
          This network processes time-series feature data as input, with the encoder transforming these features into a latent space. The temporal decoder reconstructs the features along with their predictions, whereas the label decoder extracts labels from the latent vectors. 
          The label decoder is jointly trained with the autoencoder generating latent features that improve the segmentation performance. 
        </h2>
      </div>
      
      <div class="item has-text-centered">
        <!-- Fourth image -->
        <img src="static/images/confmat_clean2.pdf" alt="MY ALT TEXT"/>
        <h2 class="subtitle">
          Confusion matrix (%) of the segmentation system on the Long-horizon demonstrations
        </h2>
      </div>
      
    </div>
  </div>
</section>
<!-- End figures -->


<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-half">
        <p class="has-text-centered">
          <!-- Your footer content here -->
          Copyright Â© 2024 DexSkills - All rights reserved
        </p>
      </div>
    </div>
  </div>
</footer>
<!-- End Footer -->

</body>
</html>
